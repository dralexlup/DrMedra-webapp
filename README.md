# DrMedra Medical Assistant

AI-powered medical assistant platform with real-time consultations, patient management, and intelligent medical insights.

## üè• Features

### ü§ñ AI-Powered Consultations
- Real-time streaming responses from medical AI models
- Voice message support with audio recording
- Image upload for visual diagnosis support
- Context-aware patient-specific consultations

### üë• Enhanced Patient Management
- Comprehensive patient records with MRN support
- Patient notes and medical history tracking
- **üìÅ File Upload**: Upload and manage patient files (images, audio, PDFs)
- **üìä Data Export**: Export patient records in JSON/TXT formats
- **üóëÔ∏è Record Management**: Delete patients and chats with confirmation dialogs

### üé® Modern User Interface
- **üåô Dark Mode**: Complete dark/light theme support across all pages
- **üì± PWA Support**: Install as mobile app with offline capabilities
- **üîÑ Real-time UI**: Live streaming responses and loading states
- **‚ôø Accessibility**: Touch-friendly, responsive design

### üîê Security & Authentication
- JWT-based authentication with Google OAuth support
- Secure file upload and storage
- Protected routes and API endpoints
- Session management with auto-refresh

## üèóÔ∏è Architecture

### Backend (FastAPI)
- **Authentication**: JWT tokens with bcrypt password hashing
- **Database**: SQLAlchemy with SQLite for patient/chat data
- **AI Integration**: OpenAI-compatible API endpoint support (vLLM, OpenAI, etc.)
- **File Upload**: Image upload and storage for visual diagnostics
- **RAG System**: Context retrieval for enhanced medical responses

### Frontend (Next.js 14)
- **Framework**: React with TypeScript and App Router
- **Authentication**: Token-based auth with localStorage
- **Real-time UI**: Live streaming chat interface
- **API Client**: Type-safe API communication
- **Responsive Design**: Mobile-friendly medical interface

## üöÄ Quick Start

### Prerequisites
- Python 3.8+ with pip
- Node.js 18+ with npm
- AI Model Endpoint (vLLM, OpenAI API, etc.)

### System Dependencies (Ubuntu/Debian)
Before installing Python packages, install required system dependencies:
```bash
# Update package list
sudo apt update

# Install required system packages for Python compilation
sudo apt install -y build-essential libpq-dev python3-dev

# Optional: Install PostgreSQL client libraries if using PostgreSQL
sudo apt install -y postgresql-client
```

**Why these packages are needed:**
- `build-essential`: Contains GCC compiler and other tools needed to compile Python packages
- `libpq-dev`: PostgreSQL development headers (required for psycopg2)
- `python3-dev`: Python development headers needed for compiling C extensions

### 1. Clone the Repository
```bash
git clone https://github.com/dralexlup/DrMedra-webapp.git
cd DrMedra-webapp
```

### 2. Backend Setup
```bash
cd api

# Make sure system dependencies are installed (see above)
# If you haven't installed them yet:
# sudo apt install -y build-essential libpq-dev python3-dev

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# or .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your settings:
# JWT_SECRET=your-secret-key
# MODEL_ENDPOINT=http://127.0.0.1:1234/v1/chat/completions
# PORT=8000

# Run the API server
python app.py
```

### 3. Frontend Setup
```bash
cd ../web

# Install dependencies
npm install

# Configure environment
cp .env.example .env.local
# Edit .env.local:
# NEXT_PUBLIC_API_BASE=http://localhost:8000

# Run the development server
npm run dev
```

### 4. AI Model Setup (vLLM Recommended)
```bash
# Install vLLM
pip install vllm

# Run with your medical model (example with Gemma)
vllm serve google/gemma-2-2b-it --host 127.0.0.1 --port 1234

# Or use any OpenAI-compatible endpoint
# Update MODEL_ENDPOINT in api/.env accordingly
```

### 5. Access the Application
- **Frontend**: http://localhost:3000
- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

## üåê Network Configuration

**Important**: The above setup works when both frontend and backend are on the same machine. For different network setups:

### Different Machines/IPs
If your backend is running on a different machine (e.g., `192.168.2.102`):

1. **Frontend configuration** (`web/.env.local`):
   ```env
   NEXT_PUBLIC_API_BASE=http://192.168.2.102:8000
   ```

2. **Backend configuration** (`api/.env`):
   ```env
   ALLOWED_ORIGINS=http://192.168.2.105:3000,http://localhost:3000
   ```
   *(Replace `192.168.2.105` with your frontend machine's IP)*

3. **Restart both services** after changing environment variables

### üìñ Detailed Network Setup Guide
For comprehensive network configuration including Docker, production, and troubleshooting:

üëâ **See [NETWORK_SETUP.md](NETWORK_SETUP.md)** for detailed configuration examples

## üìÜ Usage Guide

### Getting Started
1. **Register**: Create a doctor account at `/login` (or use demo account)
2. **Theme**: Toggle between dark/light mode using üåô/‚òÄÔ∏è button
3. **Add Patients**: Create patient records with name, MRN, and notes
4. **File Management**: Upload patient files from the patient detail page
5. **Start Consultations**: Begin general chats or patient-specific consultations
6. **AI Interactions**: Use text, voice, or image inputs for AI responses
7. **Data Export**: Export patient records using JSON or TXT format buttons
8. **Record Management**: Delete patients or individual chats as needed

### New Features Usage
- **üåô Dark Mode**: Click theme toggle on any page
- **üìÅ File Upload**: Use "Upload" button in patient files section
- **üìä Export Data**: Click "JSON" or "TXT" buttons on patient records
- **üóëÔ∏è Delete**: Use delete buttons with confirmation dialogs

### Default Login (for testing)
- Email: `admin@medra.com`
- Password: `admin123`

## üîß API Endpoints

### Authentication
- `POST /auth/register` - Register new doctor
- `POST /auth/login` - Doctor login

### Patient Management
- `GET /patients` - List all patients for doctor
- `POST /patients` - Create new patient

### Consultations
- `POST /chats` - Start new consultation
- `GET /chats` - List consultations
- `GET /messages` - Get chat history
- `POST /stream` - Stream AI responses (SSE)
- `POST /upload` - Upload medical images

### System
- `GET /health` - Health check endpoint

## üîß Configuration

### Backend Environment Variables
```env
JWT_SECRET=your-secret-key-here
MODEL_ENDPOINT=http://127.0.0.1:1234/v1/chat/completions
PORT=8000
DB_URL=sqlite:///./medra.db  # Optional, defaults to SQLite

# CORS Configuration (optional)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001  # Comma-separated
ENVIRONMENT=development  # or 'production'
PRODUCTION_ORIGINS=https://yourdomain.com,https://www.yourdomain.com
```

### CORS Configuration
The application automatically handles CORS (Cross-Origin Resource Sharing) for frontend access:

**Development Mode** (default):
- Automatically allows `localhost:3000`, `localhost:3001`, `127.0.0.1:3000`, `127.0.0.1:3001`
- Includes API documentation access on port 8000

**Production Mode**:
```env
ENVIRONMENT=production
PRODUCTION_ORIGINS=https://yourdomain.com,https://api.yourdomain.com
```

**Custom Origins**:
```env
ALLOWED_ORIGINS=http://localhost:3000,http://192.168.1.100:3000
```

**CORS Troubleshooting**:
- Test CORS: `GET http://localhost:8000/cors-test`
- Check browser console for CORS errors
- Verify frontend URL matches allowed origins exactly

### Frontend Environment Variables
```env
# For same-machine development
NEXT_PUBLIC_API_BASE=http://localhost:8000

# For network/LAN setup (replace with backend IP)
# NEXT_PUBLIC_API_BASE=http://192.168.2.102:8000

# For production
# NEXT_PUBLIC_API_BASE=https://api.yourdomain.com
```

**Important Notes:**
- Frontend environment variables must start with `NEXT_PUBLIC_`
- Restart the frontend development server after changing `.env.local`
- The URL should point to where your backend API is actually running
- In production, use HTTPS URLs

## ü§ñ Supported AI Models

### vLLM (Local Deployment)
```bash
# Medical models (examples)
vllm serve microsoft/DialoGPT-medium --host 127.0.0.1 --port 1234
vllm serve google/gemma-2-2b-it --host 127.0.0.1 --port 1234
vllm serve your-custom-medical-model --host 127.0.0.1 --port 1234
```

### OpenAI API
Update `MODEL_ENDPOINT` to `https://api.openai.com/v1/chat/completions` and add API key handling.

### Other OpenAI-Compatible APIs
- Ollama
- LocalAI
- Text Generation Inference
- Any OpenAI-compatible endpoint

## üö¢ Production Deployment

### Backend (FastAPI)
```bash
# Use production database
DB_URL=postgresql://user:password@localhost/medra_db

# Use production server
gunicorn -w 4 -k uvicorn.workers.UvicornWorker app:app --bind 0.0.0.0:8000
```

### Frontend (Next.js)
```bash
# Build for production
npm run build
npm start

# Or deploy to Vercel/Netlify
```

### Docker Deployment (Optional)
```dockerfile
# Example Dockerfile for backend
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["python", "app.py"]
```

## üîí Security Features

- ‚úÖ JWT-based authentication
- ‚úÖ Password hashing with bcrypt
- ‚úÖ SQL injection protection
- ‚úÖ CORS protection
- ‚úÖ Input validation
- ‚úÖ Secure file uploads
- ‚úÖ Environment-based secrets

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìù License

MIT License - see LICENSE file for details

## ‚ö†Ô∏è Medical Disclaimer

This system is for educational and development purposes only. Always consult qualified healthcare professionals for medical advice. The AI responses should not replace professional medical diagnosis or treatment.

---

**Built with ‚ù§Ô∏è for the medical community**